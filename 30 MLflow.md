## MLflow

- 서버 열기
	uv run python -m mlflow ui
	http://127.0.0.1:5000/


### 실험(Experiment)과 개별 실행(Run)의 단위

가장 쉬운 비유를 통해 설명해 드리겠습니다.

- **실험 (Experiment)**: **하나의 '프로젝트' 또는 '연구 주제'** 라고 생각하시면 됩니다.
    
    - 예를 들어, "고객 이탈 예측 모델 개발"이라는 큰 목표가 있다면, 이게 바로 하나의 '실험(Experiment)'이 됩니다.
        
    - 코드에서는 mlflow.set_experiment("고객_이탈_예측") 처럼 이름을 정해줍니다.
        
    - 물리적으로는 mlruns 폴더 안에 있는 숫자 이름의 폴더 하나가 하나의 실험입니다. 이 실험 폴더 안에 수많은 개별 실행(Run)들이 쌓이게 됩니다.
        
- **개별 실행 (Run)**: **프로젝트 내에서의 '하나의 시도' 또는 '하나의 레시피'** 입니다.
    
    - "고객 이탈 예측"이라는 실험 안에서, RandomForest 모델을 max_depth=10으로 돌려보는 **하나의 시도**.
        
    - LightGBM 모델을 learning_rate=0.05로 돌려보는 **또 다른 하나의 시도**.
        
    - 데이터를 오버샘플링해서 다시 RandomForest 모델을 돌려보는 **세 번째 시도**.
        
    - 이 각각의 시도들이 모두 '개별 실행(Run)'입니다.
        
    - 코드에서는 with mlflow.start_run(): 블록이 실행될 때마다 하나의 Run이 생성됩니다.
        

**요약:**  
**하나의 실험(Experiment)은 여러 개의 개별 실행(Run)들을 담는 큰 바구니입니다.**  
고객_이탈_예측 (실험)  
┣ L- rf_max_depth_10 (실행 1)  
┣ L- lgbm_lr_0.05 (실행 2)  
┣ L- rf_oversampled (실행 3)  
┗ ... (수많은 실행들)

---

### MLflow UI 처음 볼 때 순차적으로 확인하는 법

http://127.0.0.1:5000 페이지에 처음 들어오셨다면, 다음과 같은 순서로 확인하고 활용하시면 좋습니다.

#### **1단계: 실험(Experiment) 선택 및 전체 실행(Run) 목록 확인**

1. **왼쪽 사이드바 확인**: 먼저 화면 왼쪽을 보세요. 여기에 지금까지 생성된 **실험(Experiment)들의 목록**이 나옵니다. (default, Kiwi_Project_RF 등)
    
2. **관심 있는 실험 클릭**: 내가 지금 분석하고 싶은 프로젝트, 예를 들어 Kiwi_Project_RF를 클릭합니다.
    
3. **오른쪽 테이블 확인**: 클릭하면 화면 오른쪽에 해당 실험에 속한 **개별 실행(Run)들의 목록**이 표 형태로 나타납니다.
    
    - **Run Name**: with mlflow.start_run(run_name="이름")으로 지정한 이름입니다.
        
    - **Parameters**: mlflow.log_param()으로 기록한 하이퍼파라미터들(max_depth, n_estimators 등)이 보입니다.
        
    - **Metrics**: mlflow.log_metric()으로 기록한 성능 지표(accuracy, auc 등)가 보입니다.
        
    - **Source**: 어떤 파이썬 파일에서 실행했는지 나옵니다.
        

> **Tip:** 이 화면은 수많은 모델 시도 결과를 **한눈에 요약해서 보는 곳**입니다. 표의 컬럼 헤더를 클릭해서 accuracy가 가장 높은 순으로 정렬해보거나, 특정 max_depth 값을 가진 실행만 필터링해볼 수 있습니다.

#### **2단계: 특정 개별 실행(Run)의 상세 정보 파고들기**

1. **Run Name 클릭**: 1단계의 표에서 가장 성능이 좋았거나, 궁금한 실행의 **파란색 Run Name을 클릭**합니다.
    
2. **상세 페이지 확인**: 이제 그 실행 하나에 대한 모든 정보가 담긴 페이지로 이동합니다.
    
    - **Parameters (파라미터)**: 이 모델을 만들 때 사용했던 모든 하이퍼파라미터가 명확하게 기록되어 있습니다. (이제 파일명에 max_10m처럼 적을 필요가 없습니다!)
        
    - **Metrics (성능 지표)**: 이 모델의 최종 성능이 얼마였는지 보여줍니다. 만약 학습 과정(epoch)마다 기록했다면 성능 변화 그래프도 볼 수 있습니다.
        
    - **Artifacts (아티팩트)**: **가장 중요한 부분 중 하나입니다.**
        
        - mlflow.sklearn.log_model()로 저장한 **모델 파일**이 여기에 있습니다. 클릭해서 다운로드할 수도 있습니다.
            
        - 모델의 예측 결과를 시각화한 이미지(.png)나, 중요 피처 목록(feature_importance.csv) 등을 저장했다면 모두 여기서 확인할 수 있습니다.
            

> **Tip:** 이 페이지는 과거에 만들었던 특정 모델 하나를 완벽하게 **재현하고 분석하기 위한 공간**입니다. "3개월 전에 만들었던 AUC 0.85짜리 모델이 어떤 파라미터 썼더라?"라는 질문에 완벽하게 답할 수 있게 됩니다.

#### **3단계: 여러 실행(Run) 결과 직접 비교하기 (MLflow의 꽃)**

1. **다시 1단계 화면으로**: 브라우저에서 뒤로 가기를 눌러 다시 개별 실행(Run) 목록이 있는 화면으로 돌아옵니다.
    
2. **체크박스 선택**: 비교하고 싶은 실행들의 **왼쪽에 있는 체크박스를 2개 이상 선택**합니다.
    
3. **'Compare' 버튼 클릭**: 화면 위쪽에 활성화된 **'Compare' 버튼을 클릭**합니다.
    
4. **비교 페이지 확인**:
    
    - **파라미터 비교**: 여러 실행 간에 어떤 파라미터 값이 달랐는지 한눈에 보여줍니다.
        
    - **성능 비교 (시각화)**: **Parallel Coordinates Plot**이나 **Scatter Plot** 같은 차트를 통해, 어떤 파라미터를 바꿨을 때 성능(Metric)이 어떻게 변했는지 직관적으로 비교할 수 있습니다. 예를 들어, max_depth가 커질수록 accuracy가 어떻게 변하는지 관계를 쉽게 파악할 수 있습니다.
        

> **Tip:** 이 비교 기능이야말로 파일명 기반 관리 방식으로는 절대 할 수 없었던, MLflow가 제공하는 **가장 강력한 기능**입니다. 수십 개의 실험 결과를 가장 효율적으로 분석하고 인사이트를 얻는 곳입니다.

**요약된 당신의 워크플로우:**  
**실험 선택 → 성능 좋은 순으로 정렬 → 가장 좋은 실행 상세 정보 확인 → 성능이 비슷했던 다른 실행들과 체크해서 'Compare' → 어떤 파라미터가 성능에 핵심적인 영향을 미쳤는지 분석.**