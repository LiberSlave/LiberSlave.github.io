TF-IDF는 **정보 검색(information retrieval)**이나 **자연어 처리(NLP)**에서 자주 쓰이는 텍스트 가중치 기법이에요.  
문서 속 단어의 **중요도**를 수치화하는 방법이라고 생각하면 돼요.

---

### 1. 용어 정리

- **TF (Term Frequency, 단어 빈도)**
    
    - 특정 문서에서 해당 단어가 얼마나 자주 등장하는지를 나타냄.
        
    - 예: `고양이`가 문서 A에서 10번, 문서 B에서 1번 등장한다면 A에서의 TF가 더 큼.
        
    - 보통은 문서 길이에 따라 정규화(단어 수 대비 등장 횟수)하기도 함.
        
- **IDF (Inverse Document Frequency, 역문서 빈도)**
    
    - 어떤 단어가 전체 문서 집합에서 얼마나 희귀한지를 나타냄.
        
    - 자주 등장하는 단어(예: "그리고", "the")는 중요하지 않다고 보고, 드물게 등장하는 단어일수록 높은 가중치를 부여함.
        
    - 공식:
        
        IDF(t)=log⁡(Ndf(t))IDF(t) = \log \left( \frac{N}{df(t)} \right)IDF(t)=log(df(t)N​)
        - NNN: 전체 문서 수
            
        - df(t)df(t)df(t): 단어 ttt가 등장한 문서 수
            
- **TF-IDF**
    
    - 위 둘을 곱한 값:
        
        TF-IDF(t,d)=TF(t,d)×IDF(t)TF\text{-}IDF(t, d) = TF(t, d) \times IDF(t)TF-IDF(t,d)=TF(t,d)×IDF(t)
    - 즉, **특정 문서에서 자주 등장하면서도 전체 문서에서는 드문 단어일수록** 높은 점수를 가짐.
        

---

### 2. 예시

- 문서 1: "고양이가 귀엽다 고양이가 잔다"
    
- 문서 2: "강아지가 잔다"
    

👉 `고양이`

- TF(문서1): 2/4 = 0.5
    
- DF(고양이): 1 (문서1에만 등장)
    
- IDF = log(2/1) = log(2) ≈ 0.693
    
- TF-IDF(문서1, 고양이) = 0.5 × 0.693 ≈ 0.347
    

👉 `잔다`

- TF(문서1): 1/4 = 0.25
    
- TF(문서2): 1/2 = 0.5
    
- DF(잔다): 2 (모든 문서에 등장)
    
- IDF = log(2/2) = 0
    
- TF-IDF = 0 (즉, 흔한 단어라 중요하지 않다고 판단됨)
    

---

### 3. 활용 분야

- 검색 엔진(문서 검색, 키워드 매칭)
    
- 뉴스 기사나 논문에서 중요한 키워드 추출
    
- 추천 시스템
    
- 문서 유사도 계산 (코사인 유사도와 함께 많이 씀)