

- **데이터 불균형 (Imbalance)**: 주로 **분류(Classification)** 문제에서 사용되는 용어입니다. (예: 99%가 정상, 1%가 사기인 경우)
    
- **데이터 치우침 (Skewness)**: **회귀(Regression)** 문제에서 데이터 분포가 한쪽으로 길게 꼬리를 가지는 경우를 의미합니다. 사용자님의 max_10m 분포가 바로 여기에 해당합니다.
    

결과적으로 모델이 다수에만 집중하게 된다는 점은 비슷하지만, 사용하는 용어와 해결책이 약간 다릅니다.


### 데이터 치우침
---

### **1. 콴타일 회귀 (Quantile Regression): 가장 먼저 시도해야 할 최고의 선택**

#### **이것이 무엇인가요?**

- **기존 모델**: "수익률의 **평균(mean)**은 얼마일까?"를 예측합니다.
    
- **콴타일 회귀**: "**상위 10% 안에 들려면 최소한 얼마의 수익률**이 나와야 할까?" (즉, 90분위수, 90th percentile)를 예측합니다.
    

#### **왜 당신에게 가장 적합한가요?**

1. **직접적인 목표 설정**: '대박'을 예측하고 싶다는 당신의 목표와 완벽하게 일치합니다. 모델에게 "평균은 무시하고, 상위권(꼬리 부분)에만 집중해!"라고 직접적으로 지시하는 것과 같습니다.
    
2. **매우 쉬운 구현**: LightGBM과 XGBoost 모두 objective 파라미터 하나만 바꾸면 바로 사용할 수 있습니다. 복잡한 코드가 전혀 필요 없습니다.
    
3. **뛰어난 해석력**: "이 모델은 평균 수익률이 아니라, 10번 중 1번 나올 법한 '대박'의 최소 기준선을 예측합니다"라고 명확하게 설명할 수 있습니다.
    

#### **어떻게 사용하는가? (LightGBM 예시)**

codePython

```
# params 딕셔너리만 수정하면 됩니다.
params = {
    "objective": "quantile",  # <<< 'regression' 대신 'quantile'로 변경
    "alpha": 0.90,            # <<< 예측하고 싶은 분위수(quantile) 지정 (0.9 = 상위 10%)
    "n_estimators": 100,
    "random_state": 42,
    "n_jobs": -1
}

# XGBoost에서는 "objective": "reg:quantileerror", "quantile_alpha": 0.90 를 사용
```

---

### **2. 커스텀 손실 함수 (Custom Loss Function): 더 정교한 제어가 필요할 때**

#### **이것이 무엇인가요?**

모델의 '성적표 채점 방식'을 직접 만드는 것입니다.

- **기존 채점 방식 (RMSE)**: 모든 오차를 똑같이 제곱해서 감점합니다.
    
- **새로운 채점 방식**: "실제값보다 낮게 예측하는 과소 예측(Underestimation) 실수는 10점 감점, 실제값보다 높게 예측하는 과대 예측(Overestimation) 실수는 1점만 감점"과 같이 비대칭적인 패널티를 부여합니다.
    

#### **왜 당신에게 적합한가요?**

1. **유연성과 통제력**: 패널티의 강도를 원하는 대로 정밀하게 조절할 수 있습니다. 모델이 '대박'을 놓치는 실수를 얼마나 더 '아프게' 느낄지 직접 설계할 수 있습니다.
    
2. **강력한 성능**: 잘 설계된 커스텀 손실 함수는 특정 비즈니스 목표에 맞춰 모델을 최적화하여 매우 높은 성능을 낼 수 있습니다.
    

#### **단점은 무엇인가요?**

- **높은 구현 난이도**: 단순히 파라미터를 바꾸는 것이 아니라, 손실 함수의 1차 미분(gradient)과 2차 미분(hessian)을 직접 코드로 구현해야 합니다. 수학적인 이해와 복잡한 코딩이 필요합니다.




### 데이터 불균형 해결
- **분류(Classification) 문제로 변환**: "향후 10분 내 수익률이 5%를 넘을 것인가? (Yes/No)" 또는 "상위 10%의 수익률을 기록할 것인가? (Yes/No)" 와 같이 이진 분류 문제로 바꿉니다. 데이터 불균형 문제는 여전히 존재하지만, 분류 문제에서는 이를 다루는 SMOTE, Class Weight 등의 검증된 기법이 훨씬 많습니다.

