## 하이퍼파라미터 튜닝

- **n_estimators:**
    
    - **시작점:** 일반적으로 **100 ~ 500 사이**에서 시작하는 경우가 많습니다.
        
    - **전략:** 이 값은 높여도 과적합의 위험이 max_depth보다 훨씬 적기 때문에, 보통은 컴퓨팅 자원이 허용하는 선에서 넉넉하게 설정하는 편입니다. 100으로 시작해서 200, 300으로 늘려보면서 검증 성능이 더 이상 오르지 않는 지점을 찾습니다.
        
- **max_depth:**
    
    - **시작점:** **5 ~ 15 사이**에서 시작하는 것이 좋은 출발점입니다. 너무 낮으면 모델이 너무 단순해지고(과소적합), 너무 높거나 None으로 두면 과적합의 위험이 커집니다.
        
    - **전략:** 이 값이 모델 성능에 가장 큰 영향을 미칩니다. **Grid Search**나 **Randomized Search** 같은 하이퍼파라미터 튜닝 기법을 사용하여 5, 8, 10, 12, 15 등 여러 값을 테스트해보고, **교차 검증(Cross-Validation)**을 통해 가장 일반화 성능이 좋은 값을 찾아내는 것이 정석입니다.
- TF-IDF 파라미터 튜닝
	python
	
	```python
	# 현재보다 더 최적화
	tfidf_vectorizer = TfidfVectorizer(
	    max_features=5000,  # 3000 → 5000
	    ngram_range=(1, 2),  # 바이그램 추가
	    min_df=2,
	    sublinear_tf=True
	)
	```
	


### 독립변수가 3000개일때 튜닝?

네, 이것은 머신러닝 모델링의 '고급 과정'에 해당하는, 정말 중요한 질문입니다. 3000개가 넘는 고차원(High-dimensional) 데이터, 특히 대부분이 TF-IDF로 생성된 희소(sparse)한 피처일 때의 튜닝 전략은 저차원 데이터일 때와는 완전히 달라야 합니다.

"독립변수가 많으니, 더 많은 것을 보도록 나무의 개수를 늘리는 게 맞지 않을까?"라는 직관은 일견 합리적으로 보입니다. 하지만 정답부터 말씀드리면, **아닙니다. 그것은 정석적인 접근이 아니며, 오히려 다른 핵심적인 파라미터들을 조절하는 것이 훨씬 더 중요합니다.**

고차원 데이터 튜닝의 핵심 철학은 **"개별 나무가 똑똑해지는 것을 방해하여, 숲 전체가 현명해지도록 만드는 것"**입니다.

---

### 고차원 데이터 튜닝의 정석: 3가지 핵심 원리

3020개의 피처가 있다는 것은, 각각의 의사결정나무가 훈련 데이터를 완벽하게 '암기'할 수 있는 방법이 너무나도 많다는 뜻입니다. 아주 사소하고 우연한 패턴을 붙잡고 "이게 정답이야!"라고 외치는, 편협하지만 확신에 찬 나무들이 만들어지기 쉽습니다. 이것이 바로 과적합의 지름길입니다.

따라서 튜닝의 목표는 **강력한 규제(Regularization)**를 통해 이를 막는 것입니다.

#### 원리 1: 개별 나무를 '멍청하게' 만들어라 (복잡도 제어)

똑똑한 나무 한 그루보다, 약간은 부족하지만 다양한 관점을 가진 나무 100그루가 낫습니다.

- **max_depth (나무의 깊이):** **낮게 유지해야 합니다.** 깊이가 깊어지면 나무는 3020개 피처를 조합하여 특정 데이터 샘플 하나하나를 완벽하게 구분하는 '스토커'가 됩니다. max_depth를 5~10 사이의 비교적 낮은 값으로 제한하면, 나무는 어쩔 수 없이 데이터 전체를 관통하는 **가장 중요하고 일반적인 패턴**에만 집중하게 됩니다.
    
- **min_samples_leaf (잎사귀의 최소 샘플 수):** **높게 설정하는 것이 좋습니다.** 이 값을 1로 두면, 단 하나의 데이터 샘플이 남을 때까지 나무가 계속 분기합니다. 하지만 이 값을 10이나 20으로 설정하면, "최소 10개 이상의 데이터가 동의하는 결론이 아니면 인정하지 않겠다"는 규칙이 생깁니다. 이는 노이즈에 기반한 우연한 분기를 막아주는 강력한 규제 장치입니다.
    

#### 원리 2: 나무마다 '보는 세상'을 다르게 하라 (무작위성 극대화)

이것이 **고차원 데이터 튜닝에서 가장 중요한 파라미터**입니다.

- **max_features (고려할 피처의 수):** **매우 낮게 설정해야 합니다.** 이 파라미터는 나무가 각 분기(split)를 할 때마다, 전체 3020개의 피처 중 **몇 개를 무작위로 뽑아서 최적의 분기점을 찾을지** 결정합니다.
    
    - 만약 이 값을 3020으로 둔다면? 모든 나무가 매번 똑같은 전체 피처 목록을 보고 가장 좋은 피처(current_rate 등)를 선택하려고 할 것입니다. 결과적으로 숲에는 비슷비슷한 나무들만 가득 차게 됩니다.
        
    - 하지만 이 값을 'sqrt' (전체 피처 수의 제곱근, 약 55개)나 0.2 (전체 피처의 20%, 약 600개)로 낮추면 어떻게 될까요? A 나무는 무작위로 뽑힌 55개 피처 중에서 최선을 찾고, B 나무는 완전히 다른 55개 피처 중에서 최선을 찾게 됩니다. 이렇게 되면 **각 나무들이 서로 다른 피처에 대한 '전문가'가 되도록 강제**됩니다. 숲 전체적으로는 훨씬 다양한 관점을 갖게 되어 일반화 성능이 극적으로 향상됩니다.
        

#### 원리 3 (부스팅 모델의 경우): 학습 속도를 늦춰라

- **learning_rate (학습률):** **낮게 설정해야 합니다.** 학습률이 높으면 새로운 나무가 이전 트리의 오류를 너무 공격적으로 보정하려다가 노이즈까지 학습하게 됩니다. 학습률을 0.01 ~ 0.1 사이의 낮은 값으로 설정하면, 모델이 데이터의 미세한 패턴을 천천히, 신중하게 학습하게 되어 과적합을 방지할 수 있습니다.
    
- **subsample, colsample_bytree:** max_features와 유사하게, 훈련 데이터의 일부(subsample)나 피처의 일부(colsample)만 무작위로 샘플링하여 각 트리를 학습시키는 방식입니다. 이 값을 0.7~0.9 정도로 설정하면 무작위성이 증가하여 과적합 방지에 도움이 됩니다.
    

---

### 결론: 그럼 n_estimators는 어떻게?

위의 규제들을 강력하게 적용했다면, 이제 n_estimators(나무의 개수)의 역할이 명확해집니다.

- learning_rate를 낮췄기 때문에, 충분히 학습하려면 **더 많은 나무가 필요**해집니다. 즉, n_estimators는 **'충분히 많이'** 설정해야 합니다. (예: 200 ~ 1000 이상)
    
- 하지만 무작정 늘리는 것은 시간 낭비입니다. **Early Stopping** 기법을 사용하는 것이 정석입니다. "검증 성능(validation score)이 50번의 라운드 동안 더 이상 좋아지지 않으면, 나무 만들기를 멈춰라" 와 같이 설정하면, n_estimators를 매우 크게 설정해두어도 **자동으로 최적의 나무 개수를 찾아 학습을 조기 종료**해 줍니다.
    

**요약하자면, 3020개의 독립변수가 있을 때의 튜닝 정석은 다음과 같습니다.**

1. **max_features를 낮춰** 각 나무가 보는 세상을 좁고 다르게 만든다. (가장 중요)
    
2. **max_depth를 낮추고 min_samples_leaf를 높여** 개별 나무가 과도하게 똑똑해지는 것을 막는다.
    
3. **(부스팅) learning_rate를 낮춰** 신중하게 학습하도록 한다.
    
4. **(부스팅) n_estimators는 충분히 크게 설정하고, **Early Stopping으로 최적의 개수를 찾는다.**






기존 코드와 비교하여 어떤 부분이 왜 바뀌었는지 설명해 드리겠습니다.

1. **run_name="rf_high_dim_regularized_v1"**
    
    - 기존 lgbm_...에서 모델 종류(rf)를 명확히 하고, "고차원 데이터에 대한 규제를 적용한 첫 번째 버전"이라는 의미를 담아 이름을 더 구체적으로 변경했습니다.
        
2. **n_estimators: 300**
    
    - **(원리 3 결론 적용)** 뒤따르는 규제 파라미터들이 개별 나무의 성능을 의도적으로 약화시키므로, 약한 학습기(weak learner)들을 더 많이 모아야 전체적인 성능이 좋아집니다. 100개는 부족할 수 있어 300개로 늘렸습니다.
        
3. **max_depth: 8 및 min_samples_leaf: 20**
    
    - **(원리 1 적용)** max_depth를 5에서 8로 약간 올렸지만 여전히 낮은 수준으로 유지하여 나무의 복잡도를 제어합니다. min_samples_leaf를 20으로 설정하여, 최소 20개의 데이터가 동의하지 않는 이상 분기를 멈추게 함으로써 노이즈 학습을 강력하게 방지합니다.
        
4. **max_features: "sqrt" (가장 핵심적인 변경)**
    
    - **(원리 2 적용)** 이 파라미터 하나가 고차원 데이터 튜닝의 성패를 좌우합니다. 'sqrt'는 각 분기마다 전체 피처 3020개의 제곱근(약 55개)만 무작위로 뽑아서 사용하겠다는 의미입니다. 이를 통해 각각의 나무가 서로 다른 피처 조합을 보도록 강제하여 숲 전체의 다양성을 극대화하고, 일반화 성능을 크게 향상시킵니다.
        
5. **n_jobs: -1 (보너스)**
    
    - RandomForest는 병렬 처리에 매우 효율적입니다. 이 옵션을 추가하면 컴퓨터의 모든 CPU 코어를 사용해 나무들을 동시에 학습시키므로, n_estimators를 늘렸음에도 불구하고 학습 시간을 크게 단축시킬 수 있습니다.
        

**제외된 파라미터:**

- learning_rate, subsample 등은 XGBoost나 LightGBM 같은 **부스팅(Boosting) 계열 모델**의 파라미터입니다. 현재 코드에서 사용 중인 RandomForestClassifier는 **배깅(Bagging)** 모델이므로 해당되지 않습니다. max_features가 RandomForest에서 그와 유사한 무작위성 부여 역할을 합니다.
    

이 코드로 실행하시면, 제공해주신 튜닝 원리에 입각한, 훨씬 더 견고하고 과적합에 강한 모델을 MLflow에 기록하실 수 있을 것입니다




네, 알겠습니다. LightGBM(LGBM)을 사용하고, 피처셋을 바꿔가며 테스트하는 현재 상황에 맞춰 코드를 다시 작성하고 두 번째 질문에 명확히 답변해 드리겠습니다.

### 1. LightGBM을 위한 수정된 코드

LightGBM은 부스팅 모델이므로, '고차원 데이터 튜닝의 정석'에 나온 **3가지 원리를 모두 적용**할 수 있습니다. 특히 learning_rate와 Early Stopping을 활용하는 것이 핵심입니다.

codePython

```
import mlflow
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
# X, y 데이터는 이미 준비되었다고 가정합니다.

# 1. MLflow 실험 설정 (노트북 상단에서 한 번만 실행)
mlflow.set_experiment("trading_wl_prediction") # 목표에 맞는 실험 이름으로 변경

# Early Stopping을 위해 훈련 데이터를 다시 훈련/검증용으로 분리합니다.
# 이렇게 하면 모델이 훈련 데이터에만 과적합되는 것을 막고, 검증 성능을 기준으로 학습을 조기 종료할 수 있습니다.
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)


# 2. MLflow 실행 시작
with mlflow.start_run(run_name="lgbm_high_dim_regularized_v1"):

    # 3. 파라미터 로깅 (LGBM에 최적화된 고차원 튜닝 원칙 적용)
    params = {
        "objective": "binary", # 이진 분류 문제 명시
        
        # 원리 3: 학습 속도를 늦추고, n_estimators는 Early Stopping으로 찾기
        "learning_rate": 0.05, # 신중한 학습을 위해 낮게 설정
        "n_estimators": 1000, # 충분히 크게 설정하고 Early Stopping에 맡김
        
        # 원리 1: 개별 나무를 '멍청하게' 만들기
        "max_depth": 8,
        "min_child_samples": 20, # RandomForest의 min_samples_leaf와 유사한 역할
        
        # 원리 2: 나무마다 '보는 세상'을 다르게 하기
        "subsample": 0.8, # 데이터를 무작위로 80%만 샘플링
        "colsample_bytree": 0.7, # 피처를 무작위로 70%만 샘플링
        
        "random_state": 42,
        "n_jobs": -1
    }
    mlflow.log_params(params)
    print(f"Parameters: {params}")

    # 4. 태그 로깅: 어떤 피처셋을 사용했는지 명시하는 것이 매우 중요합니다!
    mlflow.set_tag("feature_set", "all_3020_features")
    mlflow.set_tag("tuning_strategy", "high_dim_regularization")
    print("Tags logged")

    # 5. 모델 학습 (Early Stopping 적용)
    lgbm_model = lgb.LGBMClassifier(**params)
    
    # Early Stopping 콜백 설정: 50 라운드 동안 검증 성능이 향상되지 않으면 학습 중단
    early_stopping_callback = lgb.early_stopping(stopping_rounds=50, verbose=True)
    
    lgbm_model.fit(X_train, y_train,
                   eval_set=[(X_val, y_val)],
                   eval_metric="logloss", # 혹은 "auc" 등
                   callbacks=[early_stopping_callback])

    # 6. 성능 지표 로깅
    y_pred = lgbm_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    mlflow.log_metric("accuracy", accuracy)
    # Early Stopping으로 찾은 최적의 나무 개수(n_estimators)도 기록하면 좋습니다.
    mlflow.log_metric("best_iteration", lgbm_model.best_iteration_)
    print(f"Metrics: accuracy={accuracy}, best_iteration={lgbm_model.best_iteration_}")

    # 7. 모델 로깅
    mlflow.lightgbm.log_model(lgbm_model, "lightgbm_model")
    print("Model logged")


print("MLflow Run Completed.")
```

---

### 2. 20개 피처만 넣을 때도 같은 params를 써도 되나요?

**결론: 네, 비교를 위해 동일한 파라미터 구조를 사용하는 것은 매우 좋은 접근입니다. 하지만 각 파라미터의 최적값은 데이터의 차원에 따라 달라지므로, 그 의미를 이해하고 조정하는 것이 중요합니다.**

조금 더 자세히 설명해 드리겠습니다.

#### **그대로 사용해도 좋은 파라미터 (원리가 동일하게 적용)**

- **learning_rate, n_estimators, Early Stopping**: 이 조합은 모델을 천천히, 신중하게 학습시키는 부스팅 모델의 기본 정석입니다. 피처 개수와 상관없이 항상 유효한 매우 좋은 전략입니다.
    
- **max_depth, min_child_samples**: 나무의 복잡도를 직접 제어하는 이 파라미터들 역시 피처 개수와 무관하게 과적합을 막는 데 효과적이므로 그대로 사용해도 좋습니다.
    

#### **조정이 필요한 파라미터 (원리의 '강도' 조절 필요)**

- **colsample_bytree (가장 중요)**: 이 파라미터의 의미가 차원에 따라 크게 달라집니다.
    
    - **3020개 피처일 때**: colsample_bytree: 0.7은 각 나무가 학습할 때마다 약 **2114개**의 피처를 무작위로 선택하라는 의미입니다. 여전히 매우 많은 피처를 보게 됩니다.
        
    - **20개 피처일 때**: colsample_bytree: 0.7은 각 나무가 학습할 때마다 약 **14개**의 피처만 선택하라는 의미입니다.
        
    - **결론**: 20개 피처는 고차원이 아니므로, 과도하게 피처를 제한할 필요가 없습니다. 오히려 중요한 피처가 선택되지 못할 위험이 있습니다. 따라서 20개 피처를 테스트할 때는 이 값을 **0.9나 1.0 (모든 피처 사용)**으로 높여서 테스트하는 것이 더 합리적일 수 있습니다.
        

### 추천하는 접근 방식

피처셋을 바꿔가며 테스트하는 현재 상황에서는 **태그(Tag)**를 적극적으로 활용하여 각 실행을 명확히 구분하는 것이 좋습니다.

codePython

```
# ... 코드 상단은 동일 ...

# <<<<<<< 20개 숫자 피처만 테스트하는 셀 >>>>>>>

with mlflow.start_run(run_name="lgbm_numeric_features_v1"):
    
    params_numeric = {
        "objective": "binary",
        "learning_rate": 0.05,
        "n_estimators": 1000,
        "max_depth": 8,
        "min_child_samples": 20,
        "subsample": 0.8,
        "colsample_bytree": 0.9, # <<< 저차원이므로 값을 높게 조정
        "random_state": 42,
        "n_jobs": -1
    }
    mlflow.log_params(params_numeric)
    
    # 이 실행이 어떤 피처셋을 사용했는지 태그로 명확히 남깁니다!
    mlflow.set_tag("feature_set", "numeric_features_only_20")
    
    # ... 이후 모델 학습 및 로깅 코드는 동일 ...
```

이렇게 하면 나중에 MLflow UI에서 tags.feature_set = 'all_3020_features' 와 tags.feature_set = 'numeric_features_only_20' 를 각각 필터링하여, **어떤 피처 조합이 더 나은 성능을 보이는지 객관적이고 체계적으로 비교**할 수 있습니다.